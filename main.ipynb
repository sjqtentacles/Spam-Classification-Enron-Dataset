{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#from the nltk corpus of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "spam_subject = []\n",
    "spam_text = []\n",
    "\n",
    "ham_subject = []\n",
    "ham_text = []\n",
    "\n",
    "#iterates through each file in enron/spam directory\n",
    "for filename in os.listdir('enron/spam/'):\n",
    "    \n",
    "    #takes out newlines in the file, each line is put into the list 'lines'\n",
    "    lines = [line.rstrip('\\n') for line in open('enron/spam/'+filename)]\n",
    "    \n",
    "    #lines[0] gets the first line, line[0][9:] \n",
    "    #gets the substring of line[0] starting from index 9.\n",
    "    #lower() makes it all lowercase\n",
    "    tempsubject = lines[0][9:].lower()\n",
    "    \n",
    "    #if the word contains only letters or numbers or both, keep it. Else, toss.\n",
    "    cleaned = [e for e in tempsubject.split(' ') if e.isalnum()]\n",
    "    \n",
    "    #if word is not useless/stopword e.g. 'is', 'the', etc. then keep it, else toss. \n",
    "    #Also toss blanks.\n",
    "    cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "    \n",
    "    #concatenates all elements of the 'cleaned' list with spaces in between the words.\n",
    "    #i.e. makes a sentence.\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    \n",
    "    if len(cleaned)>0:\n",
    "        spam_subject.append(cleaned)\n",
    "    else:\n",
    "        spam_subject.append(' ')#doing this just to have equal length vectors\n",
    "    \n",
    "    temp_spam_text = ' '#placeholder for a single message content\n",
    "    \n",
    "    #so far we have just dealt with the first line - the subject line.\n",
    "    #now we move onto the rest - the message/content/body.\n",
    "    for line in lines[1:]:\n",
    "        temptext = line.lower()\n",
    "        cleaned = [e for e in temptext.split(' ') if e.isalnum()]\n",
    "        cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "        cleaned = ' '.join(cleaned)\n",
    "        if len(cleaned)>0:\n",
    "            #put the whole message together by appending each cleaned line to the previous ones\n",
    "            temp_spam_text+=cleaned + ' '\n",
    "    \n",
    "    #finally put the cleaned entire message with no blank spaces at the ends into\n",
    "    #the spam_text list. This is the final version of the cleaned message for this file.\n",
    "    spam_text.append(temp_spam_text.strip())\n",
    "            \n",
    "for filename in os.listdir('enron/ham/'):\n",
    "    lines = [line.rstrip('\\n') for line in open('enron/ham/'+filename)]\n",
    "    tempsubject = lines[0][9:].lower()\n",
    "    cleaned = [e for e in tempsubject.split(' ') if e.isalnum()]\n",
    "    cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    if len(cleaned)>0:\n",
    "        ham_subject.append(cleaned)\n",
    "    else:\n",
    "        ham_subject.append(' ')\n",
    "    \n",
    "    temp_ham_text = ' '\n",
    "    for line in lines[1:]:\n",
    "        temptext = line.lower()\n",
    "        cleaned = [e for e in temptext.split(' ') if e.isalnum()]\n",
    "        cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "        cleaned = ' '.join(cleaned)\n",
    "        if len(cleaned)>0:\n",
    "            temp_ham_text+=cleaned+' '\n",
    "    ham_text.append(temp_ham_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16545\n",
      "16545\n",
      "17157\n",
      "17157\n"
     ]
    }
   ],
   "source": [
    "print len(ham_subject)\n",
    "print len(ham_text)\n",
    "print len(spam_subject)\n",
    "print len(spam_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I thought this was more useful instead of separate subject/message\n",
    "#thought of this a bit late so I kept the previous code anyway\n",
    "#basically put the subject and message together into the ham_subj_text list.\n",
    "ham_subj_text = []\n",
    "for i in range(len(ham_subject)):\n",
    "    ham_subj_text.append(ham_subject[i]+' '+ham_text[i])\n",
    "    \n",
    "spam_subj_text = []\n",
    "for i in range(len(spam_subject)):\n",
    "    spam_subj_text.append(spam_subject[i]+' '+spam_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33702\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>subj_text</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td>                              christmas tree farm </td>\n",
       "      <td>           christmas tree farm</td>\n",
       "      <td>                                                  </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td>                                          re thank</td>\n",
       "      <td>                            re</td>\n",
       "      <td>                                             thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1</td>\n",
       "      <td> leadership development sally timing ask shall ...</td>\n",
       "      <td>        leadership development</td>\n",
       "      <td> sally timing ask shall receive per discussion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1</td>\n",
       "      <td> key dates impact upcoming sap next weeks proje...</td>\n",
       "      <td> key dates impact upcoming sap</td>\n",
       "      <td> next weeks project apollo beyond conduct final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 1</td>\n",
       "      <td> key hr issues going year end reviews report ne...</td>\n",
       "      <td>           key hr issues going</td>\n",
       "      <td> year end reviews report needs generating like ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                          subj_text  \\\n",
       "0      1                               christmas tree farm    \n",
       "1      1                                           re thank   \n",
       "2      1  leadership development sally timing ask shall ...   \n",
       "3      1  key dates impact upcoming sap next weeks proje...   \n",
       "4      1  key hr issues going year end reviews report ne...   \n",
       "\n",
       "                         subject  \\\n",
       "0            christmas tree farm   \n",
       "1                             re   \n",
       "2         leadership development   \n",
       "3  key dates impact upcoming sap   \n",
       "4            key hr issues going   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1                                              thank  \n",
       "2  sally timing ask shall receive per discussion ...  \n",
       "3  next weeks project apollo beyond conduct final...  \n",
       "4  year end reviews report needs generating like ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new pandas dataframe with columns 'subject', 'text', 'subj_text', 'class'\n",
    "#class is the vector which contains the label for each message i.e. ham/spam\n",
    "#class is encoded as 1 for ham, 0 for spam\n",
    "data = pd.DataFrame({'subject':ham_subject+spam_subject,\n",
    "                      'text':ham_text+spam_text,\n",
    "                      'subj_text':ham_subj_text+spam_subj_text,\n",
    "                      'class':[1]*len(ham_subject)+[0]*len(spam_subject)})\n",
    "print len(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#epic feature engineering with countvectorizer - simply put, it counts the # of whatever word\n",
    "#was in your message, and puts it in the appropriate unique column.\n",
    "#I believe it's one column per word if I recall correctly. \n",
    "#VERY sparse matrix. \n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "#creates this matrix that I was writing about above\n",
    "count_vectorizer.fit(data['subj_text'].values)\n",
    "counts = count_vectorizer.transform(data['subj_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:   0.976677941962\n",
      "Precision:  0.976783846269\n",
      "Recall:     0.976677941962\n",
      "F1:         0.976674495815\n"
     ]
    }
   ],
   "source": [
    "#start making a model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "#3-fold cross-validation of the model, 5-fold is more often used but if 3-fold performs well,\n",
    "#then your model is golden.\n",
    "#clf,counts,data['class'] is just the model,data/matrix,class-labels for each row in the matrix\n",
    "#cv=3 is the number of folds in k-fold cross-validation (cross-validation = cv)\n",
    "scores = cross_val_score(clf, counts, data['class'], cv=3)\n",
    "\n",
    "#print out the average of the 3 values from the 3-fold cross-validation\n",
    "print 'Accuracy:  ', np.mean(scores)\n",
    "\n",
    "#can make precision/recall/f1/etc. with different scoring\n",
    "precisions = cross_val_score(clf, counts, data['class'], cv=3, scoring='precision_weighted')\n",
    "print 'Precision: ', np.mean(precisions)\n",
    "recalls = cross_val_score(clf, counts, data['class'], cv=3, scoring='recall_weighted')\n",
    "print 'Recall:    ', np.mean(recalls)\n",
    "f1s = cross_val_score(clf, counts, data['class'], cv=3, scoring='f1_weighted')\n",
    "print 'F1:        ', np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.99542132e-01   4.57867503e-04]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#here's an example spam message in my gmail account\n",
    "#how do I know this is spam? \n",
    "#My name wasn't mentioned, the email is from a chinese website which I had never visited\n",
    "#nothing specific about it\n",
    "#etc. etc. \n",
    "\n",
    "line = '''At the moment our team is looking for a manager in your area. We are\n",
    "looking for somebody who is ready to learn and start immediately. After\n",
    "reviewing your CV, we came to the conclusion that you are an ideal\n",
    "candidate for this job position.\n",
    "\n",
    "Our company is engaged in providing services in the area of health\n",
    "insurance. During our work, we have helped thousands of people around the\n",
    "world and we earned an irrefutable reputation. Now you have the opportunity\n",
    "to become a part of our friendly team.\n",
    "\n",
    "Position requirements:\n",
    "- You must be a US citizen.\n",
    "- Your age must be more than 21 years.\n",
    "- You must have a stable internet connection.\n",
    "- You must be willing to learn and improve.\n",
    "\n",
    "Position responsibilities:\n",
    "- Keeping your projects documentation and filling of reports.\n",
    "- Providing quality service to clients of the company.\n",
    "- Perform the tasks on time.\n",
    "- Close cooperation with other managers and our experts.\n",
    "\n",
    "Your salary will be 3000 $ per month plus 500 $ every week. Also, you will\n",
    "have the personal bonuses. If you are ready to start working, respond to\n",
    "this email. We will give you a trial period after which you can decide this\n",
    "job is right for you or not. Hope to hear from you soon.\n",
    "\n",
    "Best regards,\n",
    "Orli Irwin.'''\n",
    "\n",
    "#gotta clean it the same way I did with the training examples for it to work properly\n",
    "temptext = line.lower()\n",
    "cleaned = [e for e in temptext.split(' ') if e.isalnum()]\n",
    "cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "cleaned = ' '.join(cleaned)\n",
    "transformed = count_vectorizer.transform([cleaned])\n",
    "\n",
    "#make the model, train the model, make a prediction.\n",
    "clf = LogisticRegression()\n",
    "clf.fit(counts,data['class'])\n",
    "\n",
    "#probabilities for choosing a class. first in the array is 0's prob, next is 1's prob.\n",
    "#picks the one with the highest prob. \n",
    "print clf.predict_proba(transformed)\n",
    "\n",
    "#spits out the highest prob prediction.\n",
    "print clf.predict(transformed)\n",
    "\n",
    "#tada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
